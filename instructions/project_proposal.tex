\documentclass{acmart}
\renewcommand\footnotetextcopyrightpermission[1]{}
\setcopyright{none}
\settopmatter{printacmref=false}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\newcommand{\x}{\mathbf{x}}
\usepackage{float}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % For better tables
\usepackage{xcolor}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

 \citestyle{acmauthoryear}

%%
\title{A Large-Scale Analysis of Multi-Task, Cross-Domain Pre-training for Graph Neural Networks}
\author{Alon Bebchuk - 314023516}
\author{Ben Cohen - 323855288}
\author{Tim Kushmaro - 212052708}
\date{}

\begin{document}

\maketitle
% --------------------------------------------------
% Project-Proposal
% --------------------------------------------------

\paragraph{TL;DR} \textit{We present a large-scale, systematic analysis of multi-task, cross-domain pre-training for Graph Neural Networks (GNNs). We evaluate combinations of node, link, and graph-level objectives on a diverse set of downstream tasks, comparing against from-scratch, single-task, and single-domain baselines to establish best practices.}

% ---------- 1. Background & Motivation  (≤ 1 page) ----------
\section{Background \& Motivation}

While pre-training has revolutionized fields like NLP and Computer Vision, its application to Graph Neural Networks (GNNs) remains underdeveloped. Most GNNs are still trained from scratch for specific tasks, and existing pre-training research is often narrow, focusing on single tasks or domains \cite{hu2020strategies, you2020graph, hu2020gptgnn}. This has left a critical gap: there are no established best practices for creating general-purpose, pre-trained GNNs.

This work provides the first large-scale, systematic study of multi-task, cross-domain pre-training for GNNs. We will investigate how different combinations of pre-training tasks and data from multiple domains affect downstream performance. While recent work has explored multi-task frameworks \cite{wu2022multi, wang2022graphmvp}, no comprehensive comparison exists to identify which combinations produce the most generalizable representations. Our goal is to establish robust, transferable, and actionable guidelines for the community.

Our key contributions will be:
\begin{enumerate}
    \item \textbf{Systematic Evaluation:} A rigorous evaluation of how various combinations of node, link, and graph-level pre-training tasks impact downstream performance.
    \item \textbf{Cross-Domain Analysis:} A deep dive into the transferability of learned representations across different graph domains and task types.
    \item \textbf{Actionable Guidelines:} A set of evidence-based best practices for GNN pre-training.
    \item \textbf{Open Science:} The release of all code, pre-trained models, and experimental logs to facilitate future research.
\end{enumerate}

% ---------- 2. Proposed Approach  (≤ 1 page) ----------
\section{Proposed Approach}

Our approach is centered on a modular GNN architecture that can be pre-trained on a diverse set of tasks and then efficiently adapted to downstream applications.

\subsection{Core Architecture}
The model consists of three main components: a domain-specific input encoder, a shared GNN backbone, and task-specific prediction heads.
\begin{itemize}
    \item \textbf{Input Encoder:} A domain-specific linear encoder, \texttt{Linear(D\_in, 256) -> LayerNorm -> ReLU -> Dropout}, maps raw node features from a specific domain (with dimension $D_{in}$) into a shared 256-dimensional hidden space.
    \item \textbf{GNN Backbone:} A stack of 5 Graph Isomorphism Network (GIN) layers serves as the core representation learner. The update rule for a node $v$ from layer $l$ to $l+1$ is a four-step process:
        \begin{enumerate}
            \item \textbf{Aggregation:} $a_v^{(l)} = (1+\epsilon) h_v^{(l)} + \sum_{u \in \mathcal{N}(v)} h_u^{(l)}$
            \item \textbf{GIN Convolution:} $h'_{\text{conv}} = \text{MLP}_{\text{GIN}} ( a_v^{(l)} )$
            \item \textbf{Residual Connection:} $h'_{\text{res}} = h'_{\text{conv}} + h_v^{(l)}$
            \item \textbf{Post-Activation:} $h_v^{(l+1)} = \text{Dropout} ( \text{ReLU} ( \text{LayerNorm} ( h'_{\text{res}} ) ) )$
        \end{enumerate}
    \item \textbf{Standardized Heads:} We use simple, reusable MLP heads or decoders for various prediction tasks.
\end{itemize}

\subsection{Multi-Task Pre-training}
We will pre-train the GNN backbone using a combination of self-supervised, supervised, and adversarial objectives on a pool of graph datasets.
\begin{itemize}
    \item \textbf{Self-Supervised (Generative):}
        \begin{itemize}
            \item \textit{Node Feature Masking}: Reconstructs the initial 256-dim embeddings for 15\% of nodes whose features are masked.
            \item \textit{Link Prediction}: Classifies edges as real or fake, with one uniformly sampled negative edge for each positive edge (1:1 ratio).
        \end{itemize}
    \item \textbf{Self-Supervised (Contrastive):}
        \begin{itemize}
            \item \textit{Node-level (GraphCL-style)}: Contrasts augmented views of nodes. Augmentations include attribute masking, edge dropping, and subgraph sampling.
            \item \textit{Graph-level (InfoGraph-style)}: Maximizes mutual information between a graph's summary vector and its node embeddings,.
        \end{itemize}
    \item \textbf{Supervised Auxiliary Task:}
        \begin{itemize}
            \item \textit{Graph Property Prediction}: Regresses on z-score standardized structural properties (node count, edge count, avg. clustering coefficient).
        \end{itemize}
    \item \textbf{Domain-Adversarial Objective:} To promote domain-invariance, a gradient reversal layer (GRL) is used to train the GNN backbone to produce embeddings that confuse a domain classifier.
\end{itemize}
The tasks are combined into a single objective function where self-supervised and supervised tasks are balanced using uncertainty weighting, and the domain-adversarial loss is incorporated with a scheduled weight, $\lambda$:
\[ \mathcal{L}_{\text{total}} = \sum_{i \in \text{Tasks}} \left( \frac{1}{2\sigma_i^2}\mathcal{L}_i + \log\sigma_i \right) - \lambda \mathcal{L}_{\text{domain}} \]
The novelty of our approach lies not in any single component, but in the large-scale, systematic combination and evaluation of these methods to produce a unified framework and a set of best practices for GNN pre-training.

% ---------- 3. Experimental Plan  (≤ 1 page) ----------
\section{Experimental Plan}

Our experimental design is structured to systematically answer our core research questions regarding the effectiveness of different pre-training strategies.

\subsection{Research Questions (RQs)}
Our study is guided by the following research questions:
\begin{itemize}
    \item[\textbf{RQ1:}] When does multi-task pre-training surpass from-scratch training, and how can we mitigate negative transfer?
    \item[\textbf{RQ2:}] Which combination of pre-training tasks yields the most generalizable representations?
    \item[\textbf{RQ3:}] How can we most effectively adapt pre-trained models to downstream tasks (e.g., full fine-tuning vs. linear probing)?
    \item[\textbf{RQ4:}] Is the optimal pre-training strategy dependent on the downstream task type?
\end{itemize}

\subsection{Datasets}
\begin{itemize}
    \item \textbf{Pre-training Pool:} A combination of four graph classification datasets from TUDatasets \cite{morris2020tudataset}: \texttt{MUTAG}, \texttt{PROTEINS}, \texttt{NCI1}, and \texttt{ENZYMES}.
    \item \textbf{Downstream Tasks:} A diverse set of tasks to evaluate generalization.
        \begin{itemize}
            \item \textit{Graph Classification:} \texttt{ENZYMES} (in-domain); \texttt{FRANKENSTEIN}, \texttt{PTC\_MR} (out-of-domain).
            \item \textit{Node Classification:} \texttt{Cora}, \texttt{CiteSeer} \cite{sen2008collective}.
            \item \textit{Link Prediction:} \texttt{Cora}, \texttt{CiteSeer}.
        \end{itemize}
\end{itemize}

\subsection{Baselines and Comparisons}
We will compare a \textbf{From-Scratch} baseline against the pre-training schemes detailed in Table \ref{tab:schemes}. These schemes are designed to systematically isolate the impact of different strategies (e.g., single- vs. multi-task, generative vs. contrastive, single- vs. multi-domain) and the effect of auxiliary and adversarial objectives.

\begin{table}[H]
\caption{Experimental Pre-training Schemes}
\label{tab:schemes}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Scheme ID} & \textbf{Name} & \textbf{Tasks Included} & \textbf{Purpose \& Research Question Addressed} \\ \midrule
\textbf{B1} & From-Scratch & None & (Baseline) Establishes baseline performance. Answers \textbf{RQ1}. \\
\textbf{B2} & Single-Task (Generative) & NFM & (Baseline) A representative generative single-task model. For \textbf{RQ1 \& RQ2}. \\
\textbf{B3} & Single-Task (Contrastive) & NC & (Baseline) A representative contrastive single-task model. For \textbf{RQ1 \& RQ2}. \\
\textbf{B4} & Single-Domain (All Objectives) & NFM + LP + NC + GC + GPP & (Baseline) Isolate benefit of multi-domain data by pre-training on \texttt{ENZYMES} only. For \textbf{RQ2}. \\
\textbf{S1} & Multi-Task (Generative) & NFM + LP & Tests synergy of purely generative tasks. Addresses \textbf{RQ2}. \\
\textbf{S2} & Multi-Task (Contrastive) & NC + GC & Tests synergy of purely contrastive tasks. Addresses \textbf{RQ2}. \\
\textbf{S3} & Multi-Task (All Self-Supervised) & NFM + LP + NC + GC & Tests the synergy of all self-supervised paradigms. For \textbf{RQ2}. \\
\textbf{S4} & Multi-Task (All Objectives) & NFM + LP + NC + GC + GPP & Combines all objectives to establish a performance ceiling. For \textbf{RQ2}. \\
\textbf{S5} & Multi-Task (Domain-Invariant) & NFM + LP + NC + GC + GPP + DA & Extends the all-objective model with domain-adversarial training to improve transferability and mitigate negative transfer. For \textbf{RQ1 \& RQ2}. \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Evaluation Protocols}
Success will be measured by performance and efficiency, averaged over 3 random seeds.
\begin{itemize}
    \item \textbf{Finetuning Strategies:} We will use two methods: (1) \textbf{Full Fine-tuning}, where the GNN backbone is unfrozen, and (2) \textbf{Linear Probing}, where it is frozen. In both cases, a new prediction head is trained from scratch.
    \item \textbf{Performance Metrics:} Accuracy, F1-Score, and AUC-ROC.
    \item \textbf{Efficiency Metrics:} Convergence speed (epochs to best validation score) and wall-clock training time.
\end{itemize}

\subsection{Experimental Scope}
The study requires a significant number of runs:
\begin{itemize}
    \item \textbf{Pre-training Runs:} 8 models $\times$ 3 seeds = \textbf{24 pre-training runs}.
    \item \textbf{Finetuning Runs:} A from-scratch baseline plus 8 pre-trained models on 7 downstream tasks using 2 strategies and 3 seeds results in $(1 \times 7 \times 1 \times 3) + (8 \times 7 \times 2 \times 3) = 21 + 336 = \textbf{357 finetuning runs}$.
\end{itemize}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}